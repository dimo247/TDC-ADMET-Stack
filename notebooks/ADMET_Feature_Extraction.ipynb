{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34657ac4-c566-4721-962e-73a692e963c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found local copy...\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from tdc.single_pred import ADME\n",
    "data = ADME(name = 'VDss_Lombardo')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968c35d2-7401-4f37-b108-b1dbbfd0791b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Drug_ID', 'Drug', 'Y'], dtype='object'), (791, 3), (113, 3), (226, 3))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split['train'].columns, split['train'].shape, split['valid'].shape, split['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828f9ba3-c828-4737-8607-64c10ec3cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved to: E:\\K-MELLODDY-Project\\data\\ADMET_PK_Public_Dataset\\TDC\\ADMET_Feature_Extraction01\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define save path\n",
    "save_path = \"E:/K-MELLODDY-Project/data/ADMET_PK_Public_Dataset/TDC/ADMET_Feature_Extraction\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save each split to a CSV file\n",
    "train_path = os.path.join(save_path, \"Half_Life_Obach_train.csv\")\n",
    "valid_path = os.path.join(save_path, \"Half_Life_Obach_valid.csv\")\n",
    "test_path = os.path.join(save_path, \"Half_Life_Obach_test.csv\")\n",
    "\n",
    "split['train'].to_csv(train_path, index=False)\n",
    "split['valid'].to_csv(valid_path, index=False)\n",
    "split['test'].to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Files saved to: {os.path.abspath(save_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e997f3e-0984-4157-a3ef-0ae6d59bfb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import selfies as sf\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def build_shared_selfies_features(datasets, max_len=None):\n",
    "    \"\"\"\n",
    "    Convert multiple datasets of SMILES into SELFIES features with a shared vocabulary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : dict\n",
    "        Dictionary of name -> pd.Series of SMILES (e.g., {\"train\": train_smiles, ...})\n",
    "    max_len : int, optional\n",
    "        Maximum token length for padding/truncation. If None, will use max length across all datasets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features_dict : dict\n",
    "        Dictionary of name -> np.ndarray of indexed SELFIES tokens\n",
    "    vocab : dict\n",
    "        Shared token-to-index dictionary\n",
    "    max_len : int\n",
    "        Maximum sequence length\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert SMILES → SELFIES for all datasets\n",
    "    selfies_dict = {}\n",
    "    for name, smiles in datasets.items():\n",
    "        selfies_dict[name] = smiles.progress_apply(lambda x: sf.encoder(x) if x is not None else None).dropna()\n",
    "\n",
    "    # Step 2: Tokenize all datasets\n",
    "    tokens_dict = {name: selfies.apply(lambda s: list(sf.split_selfies(s))) \n",
    "                   for name, selfies in selfies_dict.items()}\n",
    "\n",
    "    # Step 3: Build a global vocabulary across all datasets\n",
    "    all_tokens = set()\n",
    "    for toks in tokens_dict.values():\n",
    "        for t in toks:\n",
    "            all_tokens.update(t)\n",
    "    vocab = {tok: idx + 1 for idx, tok in enumerate(sorted(all_tokens))}\n",
    "    vocab[\"[PAD]\"] = 0\n",
    "\n",
    "    # Step 4: Determine max_len\n",
    "    if max_len is None:\n",
    "        max_len = max(len(toks) for toks_list in tokens_dict.values() for toks in toks_list)\n",
    "\n",
    "    # Step 5: Convert tokens → indices with padding\n",
    "    def tokens_to_indices(toks):\n",
    "        idxs = [vocab.get(tok, 0) for tok in toks]\n",
    "        if len(idxs) < max_len:\n",
    "            idxs += [0] * (max_len - len(idxs))\n",
    "        else:\n",
    "            idxs = idxs[:max_len]\n",
    "        return idxs\n",
    "\n",
    "    features_dict = {}\n",
    "    for name, s in tokens_dict.items():\n",
    "        indices = s.progress_apply(tokens_to_indices)\n",
    "        features_dict[name] = pd.DataFrame({\n",
    "            \"SELFIES\": selfies_dict[name],\n",
    "            \"Indices\": indices\n",
    "        })    \n",
    "\n",
    "    return features_dict, vocab, max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a50064f2-7b7c-40ed-88fc-5b16c8c24942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved shared vocabulary to selfies_vocab.csv\n",
      "Processing train...\n",
      "Saved train features to features_train.csv\n",
      "Processing valid...\n",
      "Saved valid features to features_valid.csv\n",
      "Processing test...\n",
      "Saved test features to features_test.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "save_path = \"E:/K-MELLODDY-Project/data/ADMET_PK_Public_Dataset/TDC/ADMET_Feature_Extraction\"\n",
    "output_path = os.path.join(save_path, \"selfies\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Dataset splits\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Save the vocabulary as CSV\n",
    "vocab_df = pd.DataFrame(list(vocab.items()), columns=[\"Token\", \"Index\"])\n",
    "vocab_df.to_csv(os.path.join(output_path, \"selfies_vocab.csv\"), index=False)\n",
    "print(\"Saved shared vocabulary to selfies_vocab.csv\")\n",
    "\n",
    "# Process each split\n",
    "for split in splits:\n",
    "    print(f\"Processing {split}...\")\n",
    "    df = pd.read_csv(f\"{save_path}/Half_Life_Obach_{split}.csv\")\n",
    "    labels = df[\"Y\"].values  # Target labels\n",
    "\n",
    "    # Extract features from features_dict\n",
    "    feat_df = features_dict[split].copy()\n",
    "    feat_df[\"Label\"] = labels\n",
    "\n",
    "    # Expand the \"Indices\" column into separate columns for CSV\n",
    "    indices_expanded = pd.DataFrame(feat_df[\"Indices\"].tolist(),\n",
    "                                    columns=[f\"Idx_{i+1}\" for i in range(feat_df[\"Indices\"].str.len().max())])\n",
    "    \n",
    "    final_df = pd.concat([feat_df[\"SELFIES\"], indices_expanded, feat_df[\"Label\"]], axis=1)\n",
    "\n",
    "    # Save to CSV\n",
    "    final_df.to_csv(os.path.join(output_path, f\"features_{split}.csv\"), index=False)\n",
    "    print(f\"Saved {split} features to features_{split}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a2ce5d3-2b4a-4e76-b8d9-5ecca34c6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import RDLogger\n",
    "\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem.rdMolDescriptors import GetHashedMorganFingerprint\n",
    "from rdkit.Avalon.pyAvalonTools import GetAvalonCountFP\n",
    "from rdkit.Chem import rdReducedGraphs\n",
    "from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator\n",
    "\n",
    "from molfeat.trans.pretrained import PretrainedDGLTransformer\n",
    "\n",
    "class scaler:\n",
    "    def __init__(self, log=False):\n",
    "        self.log = log\n",
    "        self.offset = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def fit(self, y):\n",
    "        # make the values non-negative\n",
    "        self.offset = np.min([np.min(y), 0.0])\n",
    "        y = y.reshape(-1, 1) - self.offset\n",
    "\n",
    "        # scale the input data\n",
    "        if self.log:\n",
    "            y = np.log10(y + 1.0)\n",
    "\n",
    "        self.scaler = preprocessing.StandardScaler().fit(y)\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = y.reshape(-1, 1) - self.offset\n",
    "\n",
    "        # scale the input data\n",
    "        if self.log:\n",
    "            y = np.log10(y + 1.0)\n",
    "\n",
    "        y_scale = self.scaler.transform(y)\n",
    "\n",
    "        return y_scale\n",
    "\n",
    "    def inverse_transform(self, y_scale):\n",
    "        y = self.scaler.inverse_transform(y_scale.reshape(-1, 1))\n",
    "\n",
    "        if self.log:\n",
    "            y = 10.0**y - 1.0\n",
    "\n",
    "        y = y + self.offset\n",
    "\n",
    "        return y\n",
    "\n",
    "# from https://github.com/rdkit/rdkit/discussions/3863\n",
    "def count_to_array(fingerprint):\n",
    "    array = np.zeros((0,), dtype=np.int8)\n",
    "    \n",
    "    DataStructs.ConvertToNumpyArray(fingerprint, array)\n",
    "\n",
    "    return array\n",
    "\n",
    "def get_avalon_fingerprints(molecules, n_bits=1024):\n",
    "    fingerprints = molecules.apply(lambda x: GetAvalonCountFP(x, nBits=n_bits))\n",
    "\n",
    "    fingerprints = fingerprints.apply(count_to_array)\n",
    "    \n",
    "    return np.stack(fingerprints.values)\n",
    "\n",
    "def get_morgan_fingerprints(molecules, n_bits=1024, radius=2):\n",
    "    fingerprints = molecules.apply(lambda x: \n",
    "        GetHashedMorganFingerprint(x, nBits=n_bits, radius=radius))\n",
    "\n",
    "    fingerprints = fingerprints.apply(count_to_array)\n",
    "    \n",
    "    return np.stack(fingerprints.values)\n",
    "\n",
    "def get_erg_fingerprints(molecules):\n",
    "    fingerprints = molecules.apply(rdReducedGraphs.GetErGFingerprint)\n",
    "    \n",
    "    return np.stack(fingerprints.values)\n",
    "\n",
    "# from https://www.blopig.com/blog/2022/06/how-to-turn-a-molecule-into-a-vector-of-physicochemical-descriptors-using-rdkit/\n",
    "def get_chosen_descriptors():\n",
    "    chosen_descriptors = ['BalabanJ', 'BertzCT', 'Chi0', 'Chi0n', 'Chi0v', 'Chi1', \n",
    "        'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', \n",
    "        'EState_VSA1', 'EState_VSA10', 'EState_VSA11', 'EState_VSA2', 'EState_VSA3', \n",
    "        'EState_VSA4', 'EState_VSA5', 'EState_VSA6', 'EState_VSA7', 'EState_VSA8', \n",
    "        'EState_VSA9', 'ExactMolWt', 'FpDensityMorgan1', 'FpDensityMorgan2', \n",
    "        'FpDensityMorgan3', 'FractionCSP3', 'HallKierAlpha', 'HeavyAtomCount', \n",
    "        'HeavyAtomMolWt', 'Ipc', 'Kappa1', 'Kappa2', 'Kappa3', 'LabuteASA', \n",
    "        'MaxAbsEStateIndex', 'MaxAbsPartialCharge', 'MaxEStateIndex', 'MaxPartialCharge', \n",
    "        'MinAbsEStateIndex', 'MinAbsPartialCharge', 'MinEStateIndex', 'MinPartialCharge', \n",
    "        'MolLogP', 'MolMR', 'MolWt', 'NHOHCount', 'NOCount', 'NumAliphaticCarbocycles', \n",
    "        'NumAliphaticHeterocycles', 'NumAliphaticRings', 'NumAromaticCarbocycles', \n",
    "        'NumAromaticHeterocycles', 'NumAromaticRings', 'NumHAcceptors', 'NumHDonors', \n",
    "        'NumHeteroatoms', 'NumRadicalElectrons', 'NumRotatableBonds', \n",
    "        'NumSaturatedCarbocycles', 'NumSaturatedHeterocycles', 'NumSaturatedRings', \n",
    "        'NumValenceElectrons', 'PEOE_VSA1', 'PEOE_VSA10', 'PEOE_VSA11', 'PEOE_VSA12', \n",
    "        'PEOE_VSA13', 'PEOE_VSA14', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', \n",
    "        'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'RingCount', 'SMR_VSA1', \n",
    "        'SMR_VSA10', 'SMR_VSA2', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6', 'SMR_VSA7', \n",
    "        'SMR_VSA8', 'SMR_VSA9', 'SlogP_VSA1', 'SlogP_VSA10', 'SlogP_VSA11', 'SlogP_VSA12', \n",
    "        'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA7', \n",
    "        'SlogP_VSA8', 'SlogP_VSA9', 'TPSA', 'VSA_EState1', 'VSA_EState10', 'VSA_EState2', \n",
    "        'VSA_EState3', 'VSA_EState4', 'VSA_EState5', 'VSA_EState6', 'VSA_EState7', \n",
    "        'VSA_EState8', 'VSA_EState9', 'fr_Al_COO', 'fr_Al_OH', 'fr_Al_OH_noTert', 'fr_ArN', \n",
    "        'fr_Ar_COO', 'fr_Ar_N', 'fr_Ar_NH', 'fr_Ar_OH', 'fr_COO', 'fr_COO2', 'fr_C_O', \n",
    "        'fr_C_O_noCOO', 'fr_C_S', 'fr_HOCCN', 'fr_Imine', 'fr_NH0', 'fr_NH1', 'fr_NH2', \n",
    "        'fr_N_O', 'fr_Ndealkylation1', 'fr_Ndealkylation2', 'fr_Nhpyrrole', 'fr_SH', \n",
    "        'fr_aldehyde', 'fr_alkyl_carbamate', 'fr_alkyl_halide', 'fr_allylic_oxid', \n",
    "        'fr_amide', 'fr_amidine', 'fr_aniline', 'fr_aryl_methyl', 'fr_azide', 'fr_azo', \n",
    "        'fr_barbitur', 'fr_benzene', 'fr_benzodiazepine', 'fr_bicyclic', 'fr_diazo', \n",
    "        'fr_dihydropyridine', 'fr_epoxide', 'fr_ester', 'fr_ether', 'fr_furan', 'fr_guanido', \n",
    "        'fr_halogen', 'fr_hdrzine', 'fr_hdrzone', 'fr_imidazole', 'fr_imide', 'fr_isocyan', \n",
    "        'fr_isothiocyan', 'fr_ketone', 'fr_ketone_Topliss', 'fr_lactam', 'fr_lactone', \n",
    "        'fr_methoxy', 'fr_morpholine', 'fr_nitrile', 'fr_nitro', 'fr_nitro_arom', \n",
    "        'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_oxime', \n",
    "        'fr_para_hydroxylation', 'fr_phenol', 'fr_phenol_noOrthoHbond', 'fr_phos_acid', \n",
    "        'fr_phos_ester', 'fr_piperdine', 'fr_piperzine', 'fr_priamide', 'fr_prisulfonamd', \n",
    "        'fr_pyridine', 'fr_quatN', 'fr_sulfide', 'fr_sulfonamd', 'fr_sulfone', \n",
    "        'fr_term_acetylene', 'fr_tetrazole', 'fr_thiazole', 'fr_thiocyan', 'fr_thiophene', \n",
    "        'fr_unbrch_alkane', 'fr_urea', 'qed']\n",
    "    \n",
    "    return chosen_descriptors\n",
    "\n",
    "def get_rdkit_features(molecules):\n",
    "    calculator = MolecularDescriptorCalculator(all_descriptor_names)\n",
    "    # calculator = MolecularDescriptorCalculator(get_chosen_descriptors())\n",
    "\n",
    "    X_rdkit = molecules.apply(\n",
    "        lambda x: np.array(calculator.CalcDescriptors(x)))\n",
    "\n",
    "    X_rdkit = np.vstack(X_rdkit.values)\n",
    "\n",
    "    return X_rdkit\n",
    "\n",
    "def get_gin_supervised_masking(molecules):\n",
    "    transformer = PretrainedDGLTransformer(kind='gin_supervised_masking', dtype=float)\n",
    "\n",
    "    return transformer(molecules)\n",
    "\n",
    "\n",
    "def get_fingerprints(smiles, include_selfies=False):\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    molecules = smiles.apply(Chem.MolFromSmiles)\n",
    "\n",
    "    fingerprints = []\n",
    "\n",
    "    # Standard featurizers\n",
    "    fingerprints.append(get_morgan_fingerprints(molecules))\n",
    "    fingerprints.append(get_avalon_fingerprints(molecules))\n",
    "    fingerprints.append(get_erg_fingerprints(molecules))\n",
    "    fingerprints.append(get_rdkit_features(molecules))\n",
    "    # fingerprints.append(get_gin_supervised_masking(molecules))\n",
    "\n",
    "    return np.concatenate(fingerprints, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23f71f52-2345-4402-8fbc-25abb643d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_featurizer_shapes(smiles, include_selfies=False, max_len=None):\n",
    "    RDLogger.DisableLog('rdApp.*')\n",
    "    molecules = smiles.apply(Chem.MolFromSmiles)\n",
    "\n",
    "    print(\"Number of molecules:\", len(molecules))\n",
    "\n",
    "    # Morgan\n",
    "    morgan = get_morgan_fingerprints(molecules)\n",
    "    print(\"Morgan fingerprints:\", morgan.shape)\n",
    "\n",
    "    # Avalon\n",
    "    avalon = get_avalon_fingerprints(molecules)\n",
    "    print(\"Avalon fingerprints:\", avalon.shape)\n",
    "\n",
    "    # ErG\n",
    "    erg = get_erg_fingerprints(molecules)\n",
    "    print(\"ErG fingerprints:\", erg.shape)\n",
    "\n",
    "    # RDKit descriptors\n",
    "    rdkit_feats = get_rdkit_features(molecules)\n",
    "    print(\"RDKit descriptors:\", rdkit_feats.shape)\n",
    "\n",
    "    # # GIN embeddings\n",
    "    # gin = get_gin_supervised_masking(molecules)\n",
    "    # print(\"GIN supervised masking:\", gin.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4224af54-b634-48b3-a1c6-bc36f1d61c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train...\n",
      "Number of molecules: 791\n",
      "Morgan fingerprints: (791, 1024)\n",
      "Avalon fingerprints: (791, 1024)\n",
      "ErG fingerprints: (791, 315)\n",
      "RDKit descriptors: (791, 217)\n",
      "GIN supervised masking: (791, 300)\n",
      "Saved to: X_train.npy and y_train.npy\n",
      "Processing valid...\n",
      "Number of molecules: 113\n",
      "Morgan fingerprints: (113, 1024)\n",
      "Avalon fingerprints: (113, 1024)\n",
      "ErG fingerprints: (113, 315)\n",
      "RDKit descriptors: (113, 217)\n",
      "GIN supervised masking: (113, 300)\n",
      "Saved to: X_valid.npy and y_valid.npy\n",
      "Processing test...\n",
      "Number of molecules: 226\n",
      "Morgan fingerprints: (226, 1024)\n",
      "Avalon fingerprints: (226, 1024)\n",
      "ErG fingerprints: (226, 315)\n",
      "RDKit descriptors: (226, 217)\n",
      "GIN supervised masking: (226, 300)\n",
      "Saved to: X_test.npy and y_test.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup paths\n",
    "save_path = \"E:/K-MELLODDY-Project/data/ADMET_PK_Public_Dataset/TDC/ADMET_Feature_Extraction\"\n",
    "output_path = os.path.join(save_path, \"fingerprints\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Dataset splits\n",
    "splits = ['train', 'valid', 'test']\n",
    "\n",
    "# Process each\n",
    "for split in splits:\n",
    "    print(f\"Processing {split}...\")\n",
    "    df = pd.read_csv(f\"{save_path}/Half_Life_Obach_{split}.csv\")\n",
    "    smiles = df[\"Drug\"]\n",
    "    labels = df[\"Y\"].values  # No scaling needed\n",
    "\n",
    "    # Get fingerprints\n",
    "    debug_featurizer_shapes(smiles, include_selfies=True)\n",
    "    X = get_fingerprints(smiles)\n",
    "\n",
    "    # Save\n",
    "    np.save(f\"{output_path}/X_{split}.npy\", X)\n",
    "    np.save(f\"{output_path}/y_{split}.npy\", labels)\n",
    "    print(f\"Saved to: X_{split}.npy and y_{split}.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2db88d8-ddb0-495a-a643-ced6106e3bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((791, 2580), (791,), (113, 2580), (113,), (226, 2580), (226,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fp = np.load(save_path + \"/fingerprints/X_train.npy\")\n",
    "y_train_fp = np.load(save_path + \"/fingerprints/y_train.npy\")\n",
    "\n",
    "X_valid_fp = np.load(save_path + \"/fingerprints/X_valid.npy\")\n",
    "y_valid_fp = np.load(save_path + \"/fingerprints/y_valid.npy\")\n",
    "\n",
    "X_test_fp = np.load(save_path + \"/fingerprints/X_test.npy\")\n",
    "y_test_fp = np.load(save_path + \"/fingerprints/y_test.npy\")\n",
    "\n",
    "X_train_fp.shape, y_train_fp.shape, X_valid_fp.shape, y_valid_fp.shape, X_test_fp.shape, y_test_fp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef4b9331-dc9c-4fce-971a-a17f0eb6337f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in X_train: False\n",
      "NaNs in y_train: False\n",
      "\n",
      "NaNs in y_valid: False\n",
      "NaNs in y_valid: False\n",
      "\n",
      "NaNs in y_test: False\n",
      "NaNs in y_test: False\n"
     ]
    }
   ],
   "source": [
    "print(\"NaNs in X_train:\", np.isnan(X_train_fp).any())\n",
    "print(\"NaNs in y_train:\", np.isnan(y_train_fp).any())\n",
    "\n",
    "print(\"\\nNaNs in y_valid:\", np.isnan(X_valid_fp).any())\n",
    "print(\"NaNs in y_valid:\", np.isnan(y_valid_fp).any())\n",
    "\n",
    "print(\"\\nNaNs in y_test:\", np.isnan(X_test_fp).any())\n",
    "print(\"NaNs in y_test:\", np.isnan(y_test_fp).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922aa1f-46ef-4cd1-836d-70b041bee44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "95b75cb6-a1d4-4471-a428-1c0e6dd10c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['train', 'val', 'test'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the file\n",
    "splits = torch.load(r\"E:\\K-MELLODDY-Project\\data\\ADMET_PK_Public_Dataset\\Data_LargeMix\\pcqm4m_g25_n4\\pcqm4m_g25_n4_random_splits.pt\")\n",
    "\n",
    "print(type(splits))      # should be dict\n",
    "print(splits.keys())     # see available splits\n",
    "\n",
    "# Example: access train split\n",
    "train_data = splits[\"train\"]\n",
    "val_data = splits[\"val\"]\n",
    "test_data = splits[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dddfa8db-20c1-4073-b52d-7562f1820af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                             SMILES  assayID-1  \\\n",
      "0           1                              B.CC(=O)OC1CN2CCC1CC2        0.0   \n",
      "1           7                               B.c1ccc(N2CCOCC2)cc1        NaN   \n",
      "2          11                                          BC(=O)O.N        NaN   \n",
      "3          13                Br.Br.CCCC1CC(C)(c2csc(NCC)n2)OC1=O        NaN   \n",
      "4          14  Br.Br.CCCC1CC(C)(c2csc(NCCNc3nc(C4(C)CC(CCC)C(...        NaN   \n",
      "\n",
      "   assayID-101  assayID-103  assayID-105  assayID-107  assayID-109  \\\n",
      "0          0.0          0.0          NaN          0.0          0.0   \n",
      "1          NaN          NaN          NaN          NaN          NaN   \n",
      "2          NaN          NaN          NaN          NaN          NaN   \n",
      "3          NaN          NaN          NaN          NaN          NaN   \n",
      "4          NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "   assayID-11  assayID-113  ...  assayID-1645856  assayID-1645857  \\\n",
      "0         NaN          NaN  ...              NaN              NaN   \n",
      "1         NaN          NaN  ...              NaN              NaN   \n",
      "2         NaN          NaN  ...              NaN              NaN   \n",
      "3         NaN          NaN  ...              NaN              NaN   \n",
      "4         NaN          NaN  ...              NaN              NaN   \n",
      "\n",
      "   assayID-1645858  assayID-1645859  assayID-1645860  assayID-1671188  \\\n",
      "0              NaN              NaN              NaN              NaN   \n",
      "1              NaN              NaN              NaN              NaN   \n",
      "2              NaN              NaN              NaN              NaN   \n",
      "3              NaN              NaN              NaN              NaN   \n",
      "4              NaN              NaN              NaN              NaN   \n",
      "\n",
      "   assayID-1671193  assayID-1671194        CID        SID  \n",
      "0              NaN              NaN     376771     507893  \n",
      "1              NaN              NaN   15265529     567926  \n",
      "2              NaN              NaN   54598592     576205  \n",
      "3              NaN              NaN  126467975  333201136  \n",
      "4              NaN              NaN   52993142  333387560  \n",
      "\n",
      "[5 rows x 1332 columns]\n",
      "Index(['Unnamed: 0', 'SMILES', 'assayID-1', 'assayID-101', 'assayID-103',\n",
      "       'assayID-105', 'assayID-107', 'assayID-109', 'assayID-11',\n",
      "       'assayID-113',\n",
      "       ...\n",
      "       'assayID-1645856', 'assayID-1645857', 'assayID-1645858',\n",
      "       'assayID-1645859', 'assayID-1645860', 'assayID-1671188',\n",
      "       'assayID-1671193', 'assayID-1671194', 'CID', 'SID'],\n",
      "      dtype='object', length=1332)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load parquet file\n",
    "df = pd.read_parquet(r\"E:\\K-MELLODDY-Project\\data\\ADMET_PK_Public_Dataset\\Data_LargeMix\\pcba_1328\\PCBA_1328_1564k.parquet\")\n",
    "\n",
    "print(df.head())       # Show first rows\n",
    "print(df.columns)      # Show available columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6b851572-bca9-4566-a6d6-4be3c089a7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3810323, 31)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "865a57b1-f2ed-4310-9f93-2a3a87b4a13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1563664, 1332)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e4d9-3634-46c4-8b83-ec9c44c204fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec71e5ef-c3ba-40e9-b2d8-736f0992c329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
